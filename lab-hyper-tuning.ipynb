{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "Finally step in order to maximize the performance on your Spaceship Titanic model.\n",
    "\n",
    "The data can be found here:\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "https://github.com/data-bootcamp-v4/data/blob/main/spaceship_titanic.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been training and evaluating models with default values for hyperparameters.\n",
    "\n",
    "Today we will perform the same feature engineering as before, and then compare the best working models you got so far, but now fine tuning it's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the same as before:\n",
    "- Feature Scaling\n",
    "- Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's use the best model we got so far in order to see how it can improve when we fine tune it's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid/Random Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will use Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define hyperparameters to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Barbara/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/Barbara/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/Barbara/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Accuracy  Precision    Recall  F1 Score  Model Score  \\\n",
      "KNN                0.751620   0.751261  0.694099  0.721550     0.751620   \n",
      "Bagging            0.778258   0.759259  0.763975  0.761610     0.778258   \n",
      "Random Forest      0.799856   0.773952  0.802795  0.788110     0.799856   \n",
      "Gradient Boosting  0.794096   0.744536  0.846273  0.792151     0.794096   \n",
      "AdaBoost           0.780418   0.739745  0.812112  0.774241     0.780418   \n",
      "XGBoost            0.793377   0.742857  0.847826  0.791878     0.793377   \n",
      "\n",
      "                   Time (s)  \n",
      "KNN                0.026001  \n",
      "Bagging            0.927663  \n",
      "Random Forest      0.192249  \n",
      "Gradient Boosting  0.361823  \n",
      "AdaBoost           1.236662  \n",
      "XGBoost            0.068426  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "\n",
    "# Load dataset\n",
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")\n",
    "\n",
    "# Data preprocessing\n",
    "spaceship['CryoSleep'] = spaceship['CryoSleep'].astype(bool)\n",
    "spaceship['VIP'] = spaceship['VIP'].astype(bool)\n",
    "spaceship = spaceship.dropna()\n",
    "spaceship['Cabin'] = spaceship['Cabin'].astype(str)\n",
    "spaceship['cabin_class'] = spaceship['Cabin'].str[0]\n",
    "spaceship = spaceship.drop(columns=['PassengerId', 'Name', 'Cabin'])\n",
    "spaceship = pd.get_dummies(spaceship, columns=['HomePlanet', 'cabin_class', 'Destination'])\n",
    "spaceship['Transported'] = spaceship['Transported'].astype(int)\n",
    "\n",
    "# Feature Selection\n",
    "features = spaceship[['CryoSleep', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet_Earth', 'HomePlanet_Europa', 'HomePlanet_Mars', 'cabin_class_A', 'cabin_class_B', 'cabin_class_C', 'cabin_class_D', 'cabin_class_E', 'cabin_class_F', 'cabin_class_G', 'cabin_class_T', 'Destination_55 Cancri e', 'Destination_PSO J318.5-22', 'Destination_TRAPPIST-1e']]\n",
    "target = spaceship['Transported']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=4)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to DataFrame for consistency\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Define a function to evaluate and store model results along with computation time\n",
    "def evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "    start_time = time.time()  # Record start time\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    pred = model.predict(X_test_scaled)\n",
    "    end_time = time.time()  # Record end time\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    score = model.score(X_test_scaled, y_test)\n",
    "    computation_time = end_time - start_time  # Calculate computation time\n",
    "    \n",
    "    return accuracy, precision, recall, f1, score, computation_time\n",
    "\n",
    "# Define hyperparameters for GridSearchCV and RandomizedSearchCV for different models\n",
    "param_grids = {\n",
    "    \"BaggingClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_samples\": [0.5, 1.0],\n",
    "        \"max_features\": [0.5, 1.0]\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 30, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [3, 5, 10]\n",
    "    },\n",
    "    \"AdaBoostClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"estimator__max_depth\": [3, 5, 10]\n",
    "    },\n",
    "    \"XGBClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [3, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to get the best model using GridSearchCV and RandomizedSearchCV\n",
    "def best_model(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=5, n_jobs=-1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_grid = grid_search.best_estimator_\n",
    "    best_random = random_search.best_estimator_\n",
    "    \n",
    "    # Compare results and return the best model\n",
    "    if grid_search.best_score_ > random_search.best_score_:\n",
    "        return best_grid\n",
    "    else:\n",
    "        return best_random\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# KNN (No hyperparameter tuning for KNN in this example)\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "results['KNN'] = evaluate_model(knn, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=20), random_state=1)\n",
    "bagging_clf = best_model(bagging_clf, param_grids[\"BaggingClassifier\"], X_train_scaled, y_train)\n",
    "results['Bagging'] = evaluate_model(bagging_clf, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Random Forest Classifier\n",
    "forest_clf = RandomForestClassifier(random_state=1)\n",
    "forest_clf = best_model(forest_clf, param_grids[\"RandomForestClassifier\"], X_train_scaled, y_train)\n",
    "results['Random Forest'] = evaluate_model(forest_clf, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=1)\n",
    "gb_clf = best_model(gb_clf, param_grids[\"GradientBoostingClassifier\"], X_train_scaled, y_train)\n",
    "results['Gradient Boosting'] = evaluate_model(gb_clf, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "ada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=1), algorithm='SAMME')\n",
    "ada_clf = best_model(ada_clf, param_grids[\"AdaBoostClassifier\"], X_train_scaled, y_train)\n",
    "results['AdaBoost'] = evaluate_model(ada_clf, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgb_clf = XGBClassifier(random_state=1)\n",
    "xgb_clf = best_model(xgb_clf, param_grids[\"XGBClassifier\"], X_train_scaled, y_train)\n",
    "results['XGBoost'] = evaluate_model(xgb_clf, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Display results in a table\n",
    "results_df = pd.DataFrame(results, index=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Model Score', 'Time (s)']).T\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

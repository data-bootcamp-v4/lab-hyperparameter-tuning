{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "Finally step in order to maximize the performance on your Spaceship Titanic model.\n",
    "\n",
    "The data can be found here:\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "https://github.com/data-bootcamp-v4/data/blob/main/spaceship_titanic.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been training and evaluating models with default values for hyperparameters.\n",
    "\n",
    "Today we will perform the same feature engineering as before, and then compare the best working models you got so far, but now fine tuning it's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import time\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.datasets import  fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error, make_scorer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")\n",
    "spaceship.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the same as before:\n",
    "- Feature Scaling\n",
    "- Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's use the best model we got so far in order to see how it can improve when we fine tune it's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid/Random Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will use Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define hyperparameters to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceship.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score  # Make sure to import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "# Encoding categorical variables (CryoSleep, VIP, and Transported)\n",
    "spaceship['CryoSleep'] = spaceship['CryoSleep'].astype(int)\n",
    "spaceship['VIP'] = spaceship['VIP'].astype(int)\n",
    "spaceship['Transported'] = spaceship['Transported'].astype(int)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "X = spaceship.drop(['PassengerId', 'HomePlanet', 'Cabin', 'Destination', 'Name', 'Transported'], axis=1)\n",
    "y = spaceship['Transported']\n",
    "\n",
    "# Step 2: Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Create models\n",
    "\n",
    "# 4.1 Bagging (Using Decision Trees as base learners)\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "\n",
    "# 4.2 Pasting (Similar to Bagging, but with replacement turned off)\n",
    "pasting_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, max_samples=0.8, random_state=42)\n",
    "\n",
    "# 4.3 Random Forest (An ensemble of Decision Trees using bagging)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 4.4 Gradient Boosting (Boosting model)\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 4.5 AdaBoost (Adaptive Boosting)\n",
    "ada_model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, random_state=42)\n",
    "\n",
    "# Step 5: Train models and evaluate them\n",
    "\n",
    "# Fit and evaluate Bagging Model\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# Fit and evaluate Pasting Model\n",
    "pasting_model.fit(X_train, y_train)\n",
    "y_pred_pasting = pasting_model.predict(X_test)\n",
    "accuracy_pasting = accuracy_score(y_test, y_pred_pasting)\n",
    "\n",
    "# Fit and evaluate Random Forest Model\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Fit and evaluate Gradient Boosting Model\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "# Fit and evaluate AdaBoost Model\n",
    "ada_model.fit(X_train, y_train)\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "\n",
    "# Step 6: Compare the accuracy of each model\n",
    "print(f\"Accuracy of Bagging: {accuracy_bagging:.4f}\")\n",
    "print(f\"Accuracy of Pasting: {accuracy_pasting:.4f}\")\n",
    "print(f\"Accuracy of Random Forest: {accuracy_rf:.4f}\")\n",
    "print(f\"Accuracy of Gradient Boosting: {accuracy_gb:.4f}\")\n",
    "print(f\"Accuracy of AdaBoost: {accuracy_ada:.4f}\")\n",
    "\n",
    "# Step 7: Determine the best model\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "if accuracy_bagging > best_accuracy:\n",
    "    best_accuracy = accuracy_bagging\n",
    "    best_model_name = \"Bagging\"\n",
    "    \n",
    "if accuracy_pasting > best_accuracy:\n",
    "    best_accuracy = accuracy_pasting\n",
    "    best_model_name = \"Pasting\"\n",
    "    \n",
    "if accuracy_rf > best_accuracy:\n",
    "    best_accuracy = accuracy_rf\n",
    "    best_model_name = \"Random Forest\"\n",
    "    \n",
    "if accuracy_gb > best_accuracy:\n",
    "    best_accuracy = accuracy_gb\n",
    "    best_model_name = \"Gradient Boosting\"\n",
    "    \n",
    "if accuracy_ada > best_accuracy:\n",
    "    best_accuracy = accuracy_ada\n",
    "    best_model_name = \"AdaBoost\"\n",
    "\n",
    "print(f\"The best model is {best_model_name} with an accuracy of {best_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "Finally step in order to maximize the performance on your Spaceship Titanic model.\n",
    "\n",
    "The data can be found here:\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "https://github.com/data-bootcamp-v4/data/blob/main/spaceship_titanic.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been training and evaluating models with default values for hyperparameters.\n",
    "\n",
    "Today we will perform the same feature engineering as before, and then compare the best working models you got so far, but now fine tuning it's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "\n",
       "   Transported  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")\n",
    "spaceship.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the same as before:\n",
    "- Feature Scaling\n",
    "- Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      object\n",
       "HomePlanet       object\n",
       "CryoSleep        object\n",
       "Cabin            object\n",
       "Destination      object\n",
       "Age             float64\n",
       "VIP              object\n",
       "RoomService     float64\n",
       "FoodCourt       float64\n",
       "ShoppingMall    float64\n",
       "Spa             float64\n",
       "VRDeck          float64\n",
       "Name             object\n",
       "Transported        bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "spaceship.shape\n",
    "\n",
    "spaceship.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId       0\n",
      "HomePlanet      201\n",
      "CryoSleep       217\n",
      "Cabin           199\n",
      "Destination     182\n",
      "Age             179\n",
      "VIP             203\n",
      "RoomService     181\n",
      "FoodCourt       183\n",
      "ShoppingMall    208\n",
      "Spa             183\n",
      "VRDeck          188\n",
      "Name            200\n",
      "Transported       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "valores_perdidos = spaceship.isnull().sum()\n",
    "print(valores_perdidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
      "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
      "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
      "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
      "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
      "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
      "...          ...        ...       ...       ...            ...   ...    ...   \n",
      "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
      "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
      "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
      "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
      "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
      "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
      "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
      "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
      "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
      "...           ...        ...           ...     ...     ...                ...   \n",
      "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n",
      "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n",
      "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n",
      "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n",
      "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n",
      "\n",
      "      Transported  \n",
      "0           False  \n",
      "1            True  \n",
      "2           False  \n",
      "3           False  \n",
      "4            True  \n",
      "...           ...  \n",
      "8688        False  \n",
      "8689        False  \n",
      "8690         True  \n",
      "8691        False  \n",
      "8692         True  \n",
      "\n",
      "[6606 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "spaceship_limpio = spaceship.dropna()\n",
    "print(spaceship_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "['num__Age' 'num__RoomService' 'num__Spa' 'num__VRDeck'\n",
      " 'cat__HomePlanet_Earth' 'cat__HomePlanet_Europa' 'cat__CryoSleep_False'\n",
      " 'cat__CryoSleep_True' 'cat__Destination_55 Cancri e'\n",
      " 'cat__Destination_TRAPPIST-1e']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Assuming spaceship is your DataFrame containing the features and target variable\n",
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")  # Replace with your actual data file\n",
    "\n",
    "# Separating features and target variable\n",
    "X = spaceship.drop(columns=['Transported'])\n",
    "y = spaceship['Transported']\n",
    "\n",
    "numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n",
    "\n",
    "# Define the preprocessing steps for numerical and categorical features, including imputation\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        \n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the feature selection step\n",
    "feature_selection = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "# Create a pipeline that combines the preprocessing and feature selection\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "X_preprocessed = pipeline.fit_transform(X, y)\n",
    "\n",
    "# Display the selected feature names after one-hot encoding and scaling\n",
    "selected_features = pipeline.named_steps['feature_selection'].get_support(indices=True)\n",
    "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "selected_feature_names = feature_names[selected_features]\n",
    "\n",
    "print(\"Selected Features:\")\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "['num__Age' 'num__RoomService' 'num__Spa' 'num__VRDeck'\n",
      " 'cat__HomePlanet_Earth' 'cat__HomePlanet_Europa' 'cat__CryoSleep_False'\n",
      " 'cat__CryoSleep_True' 'cat__Destination_55 Cancri e'\n",
      " 'cat__Destination_TRAPPIST-1e']\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Assuming spaceship is your DataFrame containing the features and target variable\n",
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")  # Replace with your actual data file\n",
    "\n",
    "# Separating features and target variable\n",
    "X = spaceship.drop(columns=['Transported'])\n",
    "y = spaceship['Transported']\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identifying numerical and categorical features\n",
    "numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n",
    "\n",
    "# Define the preprocessing steps for numerical and categorical features, including imputation\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        \n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the feature selection step\n",
    "feature_selection = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "# Create a pipeline that combines the preprocessing and feature selection\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection)\n",
    "])\n",
    "# Fit the pipeline to the training data and transform both train and test sets\n",
    "X_train_preprocessed = pipeline.fit_transform(X_train, y_train)\n",
    "X_test_preprocessed = pipeline.transform(X_test)\n",
    "\n",
    "# Display the selected feature names after one-hot encoding and scaling\n",
    "selected_features = pipeline.named_steps['feature_selection'].get_support(indices=True)\n",
    "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "selected_feature_names = feature_names[selected_features]\n",
    "\n",
    "print(\"Selected Features:\")\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's use the best model we got so far in order to see how it can improve when we fine tune it's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.7573\n",
      "Pasting Accuracy: 0.7136\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "#your code here\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming spaceship is your DataFrame containing the features and target variable\n",
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")  \n",
    "\n",
    "X = spaceship.drop(columns=['Transported'])\n",
    "y = spaceship['Transported']\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline (as defined earlier)\n",
    "numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        \n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_selection = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection)\n",
    "])\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_preprocessed = pipeline.fit_transform(X_train, y_train)\n",
    "X_test_preprocessed = pipeline.transform(X_test)\n",
    "\n",
    "# Define the base estimator\n",
    "base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=base_estimator,  # Updated argument name\n",
    "    n_estimators=50,           # Number of base models\n",
    "    bootstrap=True,            # Enables bootstrapping for Bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # Use all available CPU cores\n",
    ")\n",
    "bagging_clf.fit(X_train_preprocessed, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(X_test_preprocessed)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# Pasting Classifier\n",
    "pasting_clf = BaggingClassifier(\n",
    "    estimator=base_estimator,  # Updated argument name\n",
    "    n_estimators=50,           # Number of base models\n",
    "    bootstrap=False,           # Disables bootstrapping for Pasting\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # Use all available CPU cores\n",
    ")\n",
    "pasting_clf.fit(X_train_preprocessed, y_train)\n",
    "y_pred_pasting = pasting_clf.predict(X_test_preprocessed)\n",
    "pasting_accuracy = accuracy_score(y_test, y_pred_pasting)\n",
    "\n",
    "print(f\"Bagging Accuracy: {bagging_accuracy:.4f}\")\n",
    "print(f\"Pasting Accuracy: {pasting_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.7573\n",
      "Pasting Accuracy: 0.7136\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "#your code here\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming spaceship is your DataFrame containing the features and target variable\n",
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")  \n",
    "\n",
    "X = spaceship.drop(columns=['Transported'])\n",
    "y = spaceship['Transported']\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline (as defined earlier)\n",
    "numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        \n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_selection = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection)\n",
    "])\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_preprocessed = pipeline.fit_transform(X_train, y_train)\n",
    "X_test_preprocessed = pipeline.transform(X_test)\n",
    "\n",
    "# Define the base estimator\n",
    "base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=base_estimator,  # Updated argument name\n",
    "    n_estimators=50,           # Number of base models\n",
    "    bootstrap=True,            # Enables bootstrapping for Bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # Use all available CPU cores\n",
    ")\n",
    "bagging_clf.fit(X_train_preprocessed, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(X_test_preprocessed)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# Pasting Classifier\n",
    "pasting_clf = BaggingClassifier(\n",
    "    estimator=base_estimator,  # Updated argument name\n",
    "    n_estimators=50,           # Number of base models\n",
    "    bootstrap=False,           # Disables bootstrapping for Pasting\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # Use all available CPU cores\n",
    ")\n",
    "pasting_clf.fit(X_train_preprocessed, y_train)\n",
    "y_pred_pasting = pasting_clf.predict(X_test_preprocessed)\n",
    "pasting_accuracy = accuracy_score(y_test, y_pred_pasting)\n",
    "\n",
    "print(f\"Bagging Accuracy: {bagging_accuracy:.4f}\")\n",
    "print(f\"Pasting Accuracy: {pasting_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7522\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "random_forest_clf = RandomForestClassifier(\n",
    "    n_estimators=100,   # Number of trees in the forest\n",
    "    random_state=42,\n",
    "    n_jobs=-1           # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Fit the Random Forest model to the training data\n",
    "random_forest_clf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict the target variable using the test set\n",
    "y_pred_rf = random_forest_clf.predict(X_test_preprocessed)\n",
    "\n",
    "# Calculate accuracy for the Random Forest model\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.7769\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the Gradient Boosting Classifier\n",
    "gradient_boosting_clf = GradientBoostingClassifier(\n",
    "    n_estimators=100,      # Number of boosting stages to perform\n",
    "    learning_rate=0.1,     # Step size shrinkage\n",
    "    max_depth=3,           # Maximum depth of individual trees\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Gradient Boosting model to the training data\n",
    "gradient_boosting_clf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict the target variable using the test set\n",
    "y_pred_gb = gradient_boosting_clf.predict(X_test_preprocessed)\n",
    "\n",
    "# Calculate accuracy for the Gradient Boosting model\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.7625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Casa\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the AdaBoost Classifier without a base_estimator argument\n",
    "adaboost_clf = AdaBoostClassifier(\n",
    "    n_estimators=50,      # Number of weak learners\n",
    "    learning_rate=1.0,    # Weighting of each weak learner\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the AdaBoost model to the training data\n",
    "adaboost_clf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict the target variable using the test set\n",
    "y_pred_ada = adaboost_clf.predict(X_test_preprocessed)\n",
    "\n",
    "# Calculate accuracy for the AdaBoost model\n",
    "ada_accuracy = accuracy_score(y_test, y_pred_ada)\n",
    "\n",
    "print(f\"AdaBoost Accuracy: {ada_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid/Random Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will use Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define hyperparameters to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],         # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2], # Step size\n",
    "    'max_depth': [3, 4, 5, 6],              # Depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],        # Minimum samples for splitting a node\n",
    "    'min_samples_leaf': [1, 2, 4],          # Minimum samples in a leaf\n",
    "    'subsample': [0.8, 0.9, 1.0]            # Fraction of samples used per tree\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the base model\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb_clf, \n",
    "    param_grid=param_grid,\n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    n_jobs=-1,         # Use all available cores\n",
    "    scoring='accuracy',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Extract the best model from GridSearchCV\n",
    "best_gb_clf = grid_search.best_estimator_\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_best_gb = best_gb_clf.predict(X_test_preprocessed)\n",
    "\n",
    "# Evaluate the best model's accuracy\n",
    "best_gb_accuracy = accuracy_score(y_test, y_pred_best_gb)\n",
    "\n",
    "print(f\"Best Gradient Boosting Accuracy after tuning: {best_gb_accuracy:.4f}\")\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the base model\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],     # Number of trees\n",
    "    'max_features': ['sqrt', 'log2', None],  # Number of features to consider at each split\n",
    "    'max_depth': [10, 20, 30, None],         # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum samples for splitting a node\n",
    "    'min_samples_leaf': [1, 2, 4],           # Minimum samples in a leaf\n",
    "    'bootstrap': [True, False]               # Whether to use bootstrapping\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_clf, \n",
    "    param_grid=param_grid,\n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    n_jobs=-1,         # Use all available cores\n",
    "    scoring='accuracy',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Extract the best model from GridSearchCV\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_best_rf = best_rf_clf.predict(X_test_preprocessed)\n",
    "\n",
    "# Evaluate the best model's accuracy\n",
    "best_rf_accuracy = accuracy_score(y_test, y_pred_best_rf)\n",
    "\n",
    "print(f\"Best Random Forest Accuracy after tuning: {best_rf_accuracy:.4f}\")\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
